# ğŸ“Š EmoTABIå®Ÿè¨¼å®Ÿé¨“ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆè¨­è¨ˆï¼ˆè©³ç´°ç‰ˆï¼‰

## ç›®æ¬¡
1. [ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆè¨­è¨ˆã®åŸºæœ¬æ–¹é‡](#ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆè¨­è¨ˆã®åŸºæœ¬æ–¹é‡)
2. [æ¡ç”¨ã—ãŸãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯](#æ¡ç”¨ã—ãŸãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯)
3. [è³ªå•é …ç›®ã®è©³ç´°è¨­è¨ˆ](#è³ªå•é …ç›®ã®è©³ç´°è¨­è¨ˆ)
4. [çµ±è¨ˆåˆ†ææ‰‹æ³•](#çµ±è¨ˆåˆ†ææ‰‹æ³•)
5. [åŠ¹æœåˆ¤å®šåŸºæº–](#åŠ¹æœåˆ¤å®šåŸºæº–)
6. [å®Ÿè£…ã‚³ãƒ¼ãƒ‰](#å®Ÿè£…ã‚³ãƒ¼ãƒ‰)

---

## ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆè¨­è¨ˆã®åŸºæœ¬æ–¹é‡

### æ¸¬å®šå¯¾è±¡
1. **EmoTABIã®æ¨è–¦ã®é©åˆ‡åº¦**
2. **æº€è¶³å€™è£œã¾ã§ã®åˆ°é”åº¦**
3. **ææ¡ˆè¦³å…‰åœ°ã¸ã®è¨ªå•æ„æ¬²**
4. **æ„Ÿæƒ…åˆ†æã®å¦¥å½“æ€§**
   - è‰²å½©æ„Ÿæƒ…ã®ä¸€è‡´åº¦
   - ç‰©ä½“æ„Ÿæƒ…ã®ä¸€è‡´åº¦
   - é›°å›²æ°—æ„Ÿæƒ…ã®ä¸€è‡´åº¦

### è¨­è¨ˆè¦ä»¶
- **è³ªå•æ•°**: 5å•ï¼ˆã‚µãƒ–è³ªå•å«ã‚€ï¼‰
- **å›ç­”æ™‚é–“**: 3-5åˆ†ç¨‹åº¦
- **è©•ä¾¡å°ºåº¦**: 5æ®µéšãƒªãƒƒã‚«ãƒ¼ãƒˆå°ºåº¦
- **åˆ†ææ–¹æ³•**: è¨˜è¿°çµ±è¨ˆ + æ¨æ¸¬çµ±è¨ˆ

---

## æ¡ç”¨ã—ãŸãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

### 1. ResQueï¼ˆRecommender Systems' Quality of User Experienceï¼‰
**ç”¨é€”**: Q1, Q2, Q3
- æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ å°‚ç”¨ã®è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
- çŸ¥è¦šå“è³ªã€æº€è¶³åº¦ã€è¡Œå‹•æ„å›³ã‚’ä½“ç³»çš„ã«è©•ä¾¡

### 2. ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒ»ãƒ‡ã‚£ãƒ•ã‚¡ãƒ¬ãƒ³ã‚·ãƒ£ãƒ«æ³•ï¼ˆSDæ³•ï¼‰
**ç”¨é€”**: Q4
- æ„Ÿæƒ…ãƒ»å°è±¡ã®æ¸¬å®šã«æœ€é©
- 3ã¤ã®æ„Ÿæƒ…åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã®å¦¥å½“æ€§è©•ä¾¡

### 3. System Usability Scaleï¼ˆSUSï¼‰ã®è¦ç´ 
**ç”¨é€”**: Q5
- ç·åˆæº€è¶³åº¦ã®æ¸¬å®š

### 4. Technology Acceptance Modelï¼ˆTAMï¼‰ã®è¦ç´ 
**ç”¨é€”**: Q3, Q5
- æŠ€è¡“å—å®¹åº¦ã®æ¸¬å®š

---

## è³ªå•é …ç›®ã®è©³ç´°è¨­è¨ˆ

### Q1: æ¨è–¦ã®é©åˆ‡åº¦

#### ğŸ“‹ ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆæ‰‹æ³•
**ResQueï¼ˆæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ å“è³ªè©•ä¾¡ï¼‰**

#### ğŸ“ è³ªå•æ–‡
```
Q1. EmoTABIãŒææ¡ˆã—ãŸæ—…è¡Œå…ˆã¯ã€ã‚ãªãŸãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸå†™çœŸã®
    é›°å›²æ°—ã‚„æ°—åˆ†ã«åˆã£ã¦ã„ã¾ã—ãŸã‹ï¼Ÿ

â–¡ 1 - å…¨ãåˆã£ã¦ã„ãªã„
â–¡ 2 - ã‚ã¾ã‚Šåˆã£ã¦ã„ãªã„
â–¡ 3 - ã©ã¡ã‚‰ã¨ã‚‚ã„ãˆãªã„
â–¡ 4 - ã‚„ã‚„åˆã£ã¦ã„ã‚‹
â–¡ 5 - éå¸¸ã«åˆã£ã¦ã„ã‚‹
```

#### ğŸ“Š çµæœåˆ†ææ–¹æ³•

**1. è¨˜è¿°çµ±è¨ˆ**
```python
# å¹³å‡å€¤ã€ä¸­å¤®å€¤ã€æ¨™æº–åå·®
mean_q1 = df['Q1'].mean()
median_q1 = df['Q1'].median()
std_q1 = df['Q1'].std()

# è‚¯å®šç‡ï¼ˆã‚¹ã‚³ã‚¢4ä»¥ä¸Šã®å‰²åˆï¼‰
positive_rate = (df['Q1'] >= 4).sum() / len(df) * 100
```

**2. æ¨æ¸¬çµ±è¨ˆ**
```python
# 1ã‚µãƒ³ãƒ—ãƒ«tæ¤œå®šï¼ˆåŸºæº–å€¤3.0ã¨ã®æ¯”è¼ƒï¼‰
from scipy import stats
t_stat, p_value = stats.ttest_1samp(df['Q1'], 3.0)

# åŠ¹æœé‡ï¼ˆCohen's dï¼‰
cohens_d = (df['Q1'].mean() - 3.0) / df['Q1'].std()
```

**3. ã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒ**
```python
# ç”»åƒã‚°ãƒ«ãƒ¼ãƒ—é–“ã®1è¦å› åˆ†æ•£åˆ†æ
group1 = df[df['ä½¿ç”¨ç”»åƒ'] == 1]['Q1']
group2 = df[df['ä½¿ç”¨ç”»åƒ'] == 2]['Q1']
group3 = df[df['ä½¿ç”¨ç”»åƒ'] == 3]['Q1']

f_stat, p_value = stats.f_oneway(group1, group2, group3)
```

#### âœ… åŠ¹æœã‚ã‚Šã¨åˆ¤æ–­ã§ãã‚‹åŸºæº–

| æŒ‡æ¨™ | æœ€ä½åŸºæº– | ç›®æ¨™å€¤ | å„ªç§€åŸºæº– |
|------|---------|--------|---------|
| **å¹³å‡ã‚¹ã‚³ã‚¢** | 3.5ä»¥ä¸Š | 3.8ä»¥ä¸Š | 4.0ä»¥ä¸Š |
| **è‚¯å®šç‡** | 60%ä»¥ä¸Š | 70%ä»¥ä¸Š | 80%ä»¥ä¸Š |
| **æ¨™æº–åå·®** | 1.0ä»¥ä¸‹ | 0.8ä»¥ä¸‹ | 0.6ä»¥ä¸‹ |
| **tæ¤œå®š på€¤** | < 0.05 | < 0.01 | < 0.001 |
| **åŠ¹æœé‡ d** | 0.5ä»¥ä¸Š | 0.8ä»¥ä¸Š | 1.0ä»¥ä¸Š |

**çµ±è¨ˆçš„åˆ¤å®š**:
- p < 0.05 ã‹ã¤ tå€¤ãŒæ­£ â†’ **çµ±è¨ˆçš„ã«æœ‰æ„ã«åŠ¹æœã‚ã‚Š**
- Cohen's d â‰¥ 0.5 â†’ **å®Ÿè³ªçš„ã«ä¸­ç¨‹åº¦ä»¥ä¸Šã®åŠ¹æœ**

---

### Q2: æº€è¶³å€™è£œã¸ã®åˆ°é”åº¦

#### ğŸ“‹ ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆæ‰‹æ³•
**ResQue + ã‚¿ã‚¹ã‚¯å®Œäº†åº¦è©•ä¾¡**

#### ğŸ“ è³ªå•æ–‡
```
Q2. ææ¡ˆã•ã‚ŒãŸæ—…è¡Œå…ˆã®ä¸­ã«ã€ã‚ãªãŸãŒã€Œè¡Œã£ã¦ã¿ãŸã„ã€ã¨
    æº€è¶³ã§ãã‚‹å€™è£œã¯ã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ

â–¡ 1 - å…¨ããªã‹ã£ãŸï¼ˆ0ä»¶ï¼‰
â–¡ 2 - ã»ã¨ã‚“ã©ãªã‹ã£ãŸï¼ˆ1ä»¶ç¨‹åº¦ï¼‰
â–¡ 3 - ã„ãã¤ã‹ã‚ã£ãŸï¼ˆ2-3ä»¶ç¨‹åº¦ï¼‰
â–¡ 4 - å¤šãã‚ã£ãŸï¼ˆ4-5ä»¶ç¨‹åº¦ï¼‰
â–¡ 5 - ã»ã¼å…¨ã¦æº€è¶³ã§ããŸï¼ˆ6ä»¶ä»¥ä¸Šï¼‰
```

#### ğŸ“Š çµæœåˆ†ææ–¹æ³•

**1. ã‚¿ã‚¹ã‚¯æˆåŠŸç‡**
```python
# ã‚¿ã‚¹ã‚¯æˆåŠŸç‡ï¼ˆã‚¹ã‚³ã‚¢3ä»¥ä¸Šï¼‰
task_success_rate = (df['Q2'] >= 3).sum() / len(df) * 100

# é«˜æº€è¶³ç‡ï¼ˆã‚¹ã‚³ã‚¢4ä»¥ä¸Šï¼‰
high_satisfaction_rate = (df['Q2'] >= 4).sum() / len(df) * 100
```

**2. äºŒé …æ¤œå®š**
```python
# ã‚¿ã‚¹ã‚¯æˆåŠŸç‡ãŒ50%ã‚ˆã‚Šæœ‰æ„ã«é«˜ã„ã‹
from statsmodels.stats.proportion import binom_test
n_success = (df['Q2'] >= 3).sum()
p_value = binom_test(n_success, len(df), prop=0.5, alternative='larger')
```

**3. åˆ°é”åº¦ãƒ¬ãƒ™ãƒ«åˆ†é¡**
```python
# ãƒ¬ãƒ™ãƒ«åˆ†é¡
df['åˆ°é”ãƒ¬ãƒ™ãƒ«'] = pd.cut(df['Q2'], 
                         bins=[0, 2, 3, 5], 
                         labels=['å¤±æ•—', 'éƒ¨åˆ†æˆåŠŸ', 'æˆåŠŸ'])

# å„ãƒ¬ãƒ™ãƒ«ã®å‰²åˆ
level_distribution = df['åˆ°é”ãƒ¬ãƒ™ãƒ«'].value_counts(normalize=True) * 100
```

#### âœ… åŠ¹æœã‚ã‚Šã¨åˆ¤æ–­ã§ãã‚‹åŸºæº–

| æŒ‡æ¨™ | æœ€ä½åŸºæº– | ç›®æ¨™å€¤ | å„ªç§€åŸºæº– |
|------|---------|--------|---------|
| **ã‚¿ã‚¹ã‚¯æˆåŠŸç‡** | 70%ä»¥ä¸Š | 80%ä»¥ä¸Š | 90%ä»¥ä¸Š |
| **é«˜æº€è¶³ç‡** | 40%ä»¥ä¸Š | 50%ä»¥ä¸Š | 60%ä»¥ä¸Š |
| **å¹³å‡ã‚¹ã‚³ã‚¢** | 3.0ä»¥ä¸Š | 3.5ä»¥ä¸Š | 4.0ä»¥ä¸Š |
| **äºŒé …æ¤œå®š på€¤** | < 0.05 | < 0.01 | < 0.001 |

**çµ±è¨ˆçš„åˆ¤å®š**:
- ã‚¿ã‚¹ã‚¯æˆåŠŸç‡ãŒ50%ã‚ˆã‚Šæœ‰æ„ã«é«˜ã„ï¼ˆp < 0.05ï¼‰â†’ **åŠ¹æœã‚ã‚Š**

---

### Q3: è¡Œå‹•æ„å›³

#### ğŸ“‹ ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆæ‰‹æ³•
**ResQueï¼ˆè¡Œå‹•æ„å›³ï¼‰+ TAM**

#### ğŸ“ è³ªå•æ–‡
```
Q3. EmoTABIãŒææ¡ˆã—ãŸæ—…è¡Œå…ˆã«ã€å®Ÿéš›ã«è¡Œã£ã¦ã¿ãŸã„ã¨æ€ã„ã¾ã™ã‹ï¼Ÿ

â–¡ 1 - å…¨ãæ€ã‚ãªã„
â–¡ 2 - ã‚ã¾ã‚Šæ€ã‚ãªã„
â–¡ 3 - ã©ã¡ã‚‰ã¨ã‚‚ã„ãˆãªã„
â–¡ 4 - ã‚„ã‚„æ€ã†
â–¡ 5 - å¼·ãæ€ã†
```

#### ğŸ“Š çµæœåˆ†ææ–¹æ³•

**1. è¡Œå‹•æ„å›³ç‡**
```python
# è¡Œå‹•æ„å›³ç‡ï¼ˆã‚¹ã‚³ã‚¢4ä»¥ä¸Šï¼‰
intention_rate = (df['Q3'] >= 4).sum() / len(df) * 100

# å¼·æ„å›³ç‡ï¼ˆã‚¹ã‚³ã‚¢5ï¼‰
strong_intention_rate = (df['Q3'] == 5).sum() / len(df) * 100
```

**2. æ¨è–¦å“è³ªã¨ã®ç›¸é–¢åˆ†æ**
```python
# Q1ï¼ˆæ¨è–¦é©åˆ‡åº¦ï¼‰ã¨Q3ï¼ˆè¡Œå‹•æ„å›³ï¼‰ã®ç›¸é–¢
from scipy.stats import pearsonr
corr, p_value = pearsonr(df['Q1'], df['Q3'])
```

**3. NPSé¢¨ã‚¹ã‚³ã‚¢**
```python
# Net Promoter Scoreçš„ãªæŒ‡æ¨™
promoters = (df['Q3'] == 5).sum() / len(df) * 100
detractors = (df['Q3'] <= 2).sum() / len(df) * 100
nps_score = promoters - detractors
```

#### âœ… åŠ¹æœã‚ã‚Šã¨åˆ¤æ–­ã§ãã‚‹åŸºæº–

| æŒ‡æ¨™ | æœ€ä½åŸºæº– | ç›®æ¨™å€¤ | å„ªç§€åŸºæº– |
|------|---------|--------|---------|
| **è¡Œå‹•æ„å›³ç‡** | 50%ä»¥ä¸Š | 60%ä»¥ä¸Š | 70%ä»¥ä¸Š |
| **å¼·æ„å›³ç‡** | 20%ä»¥ä¸Š | 30%ä»¥ä¸Š | 40%ä»¥ä¸Š |
| **å¹³å‡ã‚¹ã‚³ã‚¢** | 3.5ä»¥ä¸Š | 3.8ä»¥ä¸Š | 4.0ä»¥ä¸Š |
| **Q1ã¨ã®ç›¸é–¢ä¿‚æ•°** | r > 0.5 | r > 0.6 | r > 0.7 |
| **ç›¸é–¢ã®på€¤** | < 0.05 | < 0.01 | < 0.001 |
| **NPSé¢¨ã‚¹ã‚³ã‚¢** | 20ä»¥ä¸Š | 30ä»¥ä¸Š | 40ä»¥ä¸Š |

**çµ±è¨ˆçš„åˆ¤å®š**:
- Q1ã¨Q3ã®ç›¸é–¢ãŒæœ‰æ„ï¼ˆp < 0.05ï¼‰ã‹ã¤ä¸­ç¨‹åº¦ä»¥ä¸Šï¼ˆr > 0.5ï¼‰
  â†’ **æ¨è–¦å“è³ªãŒè¡Œå‹•æ„å›³ã«å½±éŸ¿ã—ã¦ã„ã‚‹**

---

### Q4: æ„Ÿæƒ…åˆ†æã®å¦¥å½“æ€§

#### ğŸ“‹ ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆæ‰‹æ³•
**ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒ»ãƒ‡ã‚£ãƒ•ã‚¡ãƒ¬ãƒ³ã‚·ãƒ£ãƒ«æ³•ï¼ˆSDæ³•ï¼‰æ”¹å¤‰ç‰ˆ**

#### ğŸ“ è³ªå•æ–‡
```
Q4. ã‚ãªãŸãŒé¸æŠã—ãŸå†™çœŸã«ã¤ã„ã¦ã€EmoTABIã®åˆ†æçµæœã¯
    å†™çœŸã®å°è±¡ã¨ä¸€è‡´ã—ã¦ã„ã¾ã—ãŸã‹ï¼Ÿ
    ä»¥ä¸‹ã®Aã€œCãã‚Œãã‚Œã«ã¤ã„ã¦ãŠç­”ãˆãã ã•ã„ã€‚

ã€å‰æã€‘ä½¿ç”¨ã—ãŸç”»åƒç•ªå·ï¼š â–¡ ç”»åƒ1  â–¡ ç”»åƒ2  â–¡ ç”»åƒ3

A. è‰²å½©æ„Ÿæƒ…ï¼ˆå†™çœŸã®è‰²ã‹ã‚‰åˆ†æã•ã‚ŒãŸæ„Ÿæƒ…ï¼‰
   ä¾‹ï¼šã€Œæš–ã‹ã„ã€ã€Œæ˜ã‚‹ã„ã€ã€Œè½ã¡ç€ã„ãŸã€ãªã©
   
   ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ†æã—ãŸè‰²å½©æ„Ÿæƒ…ï¼š__________ï¼ˆè¡¨ç¤ºã•ã‚Œã‚‹ï¼‰
   
â–¡ 1 - å…¨ãä¸€è‡´ã—ãªã„
â–¡ 2 - ã‚ã¾ã‚Šä¸€è‡´ã—ãªã„
â–¡ 3 - ã©ã¡ã‚‰ã¨ã‚‚ã„ãˆãªã„
â–¡ 4 - ã‚„ã‚„ä¸€è‡´ã™ã‚‹
â–¡ 5 - éå¸¸ã«ä¸€è‡´ã™ã‚‹

B. ç‰©ä½“æ„Ÿæƒ…ï¼ˆå†™ã£ã¦ã„ã‚‹ç‰©ã‹ã‚‰åˆ†æã•ã‚ŒãŸæ„Ÿæƒ…ï¼‰
   ä¾‹ï¼šã€Œè‡ªç„¶è±Šã‹ã€ã€Œéƒ½ä¼šçš„ã€ã€Œæ­´å²çš„ã€ãªã©
   
   ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ†æã—ãŸç‰©ä½“æ„Ÿæƒ…ï¼š__________ï¼ˆè¡¨ç¤ºã•ã‚Œã‚‹ï¼‰
   
â–¡ 1 - å…¨ãä¸€è‡´ã—ãªã„
â–¡ 2 - ã‚ã¾ã‚Šä¸€è‡´ã—ãªã„
â–¡ 3 - ã©ã¡ã‚‰ã¨ã‚‚ã„ãˆãªã„
â–¡ 4 - ã‚„ã‚„ä¸€è‡´ã™ã‚‹
â–¡ 5 - éå¸¸ã«ä¸€è‡´ã™ã‚‹

C. é›°å›²æ°—æ„Ÿæƒ…ï¼ˆå†™çœŸå…¨ä½“ã®é›°å›²æ°—ã‹ã‚‰åˆ†æã•ã‚ŒãŸæ„Ÿæƒ…ï¼‰
   ä¾‹ï¼šã€Œãƒªãƒ©ãƒƒã‚¯ã‚¹ã€ã€Œå†’é™ºå¿ƒã€ã€Œãƒã‚¹ã‚¿ãƒ«ã‚¸ãƒƒã‚¯ã€ãªã©
   
   ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ†æã—ãŸé›°å›²æ°—æ„Ÿæƒ…ï¼š__________ï¼ˆè¡¨ç¤ºã•ã‚Œã‚‹ï¼‰
   
â–¡ 1 - å…¨ãä¸€è‡´ã—ãªã„
â–¡ 2 - ã‚ã¾ã‚Šä¸€è‡´ã—ãªã„
â–¡ 3 - ã©ã¡ã‚‰ã¨ã‚‚ã„ãˆãªã„
â–¡ 4 - ã‚„ã‚„ä¸€è‡´ã™ã‚‹
â–¡ 5 - éå¸¸ã«ä¸€è‡´ã™ã‚‹
```

#### ğŸ“Š çµæœåˆ†ææ–¹æ³•

**1. å„ã‚¨ãƒ³ã‚¸ãƒ³ã®ç²¾åº¦è©•ä¾¡**
```python
# å„æ„Ÿæƒ…åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã®å¹³å‡ã‚¹ã‚³ã‚¢
q4a_mean = df['Q4_A'].mean()  # è‰²å½©æ„Ÿæƒ…
q4b_mean = df['Q4_B'].mean()  # ç‰©ä½“æ„Ÿæƒ…
q4c_mean = df['Q4_C'].mean()  # é›°å›²æ°—æ„Ÿæƒ…

# å„ã‚¨ãƒ³ã‚¸ãƒ³ã®ä¸€è‡´ç‡ï¼ˆã‚¹ã‚³ã‚¢4ä»¥ä¸Šï¼‰
q4a_accuracy = (df['Q4_A'] >= 4).sum() / len(df) * 100
q4b_accuracy = (df['Q4_B'] >= 4).sum() / len(df) * 100
q4c_accuracy = (df['Q4_C'] >= 4).sum() / len(df) * 100

# ç·åˆå¦¥å½“æ€§ã‚¹ã‚³ã‚¢
q4_total = (q4a_mean + q4b_mean + q4c_mean) / 3
```

**2. ã‚¨ãƒ³ã‚¸ãƒ³é–“ã®æ¯”è¼ƒï¼ˆFriedmanæ¤œå®šï¼‰**
```python
from scipy.stats import friedmanchisquare

# å¯¾å¿œã‚ã‚Š3ç¾¤æ¯”è¼ƒ
stat, p_value = friedmanchisquare(df['Q4_A'], df['Q4_B'], df['Q4_C'])

# æœ‰æ„å·®ãŒã‚ã‚‹å ´åˆã®äº‹å¾Œæ¤œå®š
if p_value < 0.05:
    from scipy.stats import wilcoxon
    
    # Bonferroniè£œæ­£ï¼ˆÎ± = 0.05 / 3 = 0.0167ï¼‰
    alpha_corrected = 0.05 / 3
    
    # A vs B
    stat_ab, p_ab = wilcoxon(df['Q4_A'], df['Q4_B'])
    
    # A vs C
    stat_ac, p_ac = wilcoxon(df['Q4_A'], df['Q4_C'])
    
    # B vs C
    stat_bc, p_bc = wilcoxon(df['Q4_B'], df['Q4_C'])
```

**3. å„ã‚¨ãƒ³ã‚¸ãƒ³ã®åŸºæº–å€¤ã¨ã®æ¯”è¼ƒ**
```python
# å„ã‚¨ãƒ³ã‚¸ãƒ³ãŒåŸºæº–å€¤3.5ä»¥ä¸Šã‹ï¼ˆ1ã‚µãƒ³ãƒ—ãƒ«tæ¤œå®šï¼‰
for label, col in [('è‰²å½©', 'Q4_A'), ('ç‰©ä½“', 'Q4_B'), ('é›°å›²æ°—', 'Q4_C')]:
    t_stat, p_value = stats.ttest_1samp(df[col], 3.5)
    print(f"{label}: t={t_stat:.3f}, p={p_value:.4f}")
```

**4. ç”»åƒåˆ¥ã®å¦¥å½“æ€§åˆ†æ**
```python
# ç”»åƒã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®åˆ†æ
for img_id in [1, 2, 3]:
    img_data = df[df['ä½¿ç”¨ç”»åƒ'] == img_id]
    print(f"ç”»åƒ{img_id}:")
    print(f"  è‰²å½©: {img_data['Q4_A'].mean():.2f}")
    print(f"  ç‰©ä½“: {img_data['Q4_B'].mean():.2f}")
    print(f"  é›°å›²æ°—: {img_data['Q4_C'].mean():.2f}")
```

#### âœ… åŠ¹æœã‚ã‚Šã¨åˆ¤æ–­ã§ãã‚‹åŸºæº–

**å„ã‚¨ãƒ³ã‚¸ãƒ³å€‹åˆ¥åŸºæº–**

| ã‚¨ãƒ³ã‚¸ãƒ³ | å¹³å‡ã‚¹ã‚³ã‚¢ï¼ˆæœ€ä½ï¼‰ | å¹³å‡ã‚¹ã‚³ã‚¢ï¼ˆç›®æ¨™ï¼‰ | ä¸€è‡´ç‡ï¼ˆæœ€ä½ï¼‰ | ä¸€è‡´ç‡ï¼ˆç›®æ¨™ï¼‰ |
|---------|------------------|------------------|---------------|---------------|
| **è‰²å½©æ„Ÿæƒ…** | 3.5ä»¥ä¸Š | 4.0ä»¥ä¸Š | 60%ä»¥ä¸Š | 75%ä»¥ä¸Š |
| **ç‰©ä½“æ„Ÿæƒ…** | 3.3ä»¥ä¸Š | 3.8ä»¥ä¸Š | 55%ä»¥ä¸Š | 70%ä»¥ä¸Š |
| **é›°å›²æ°—æ„Ÿæƒ…** | 3.7ä»¥ä¸Š | 4.2ä»¥ä¸Š | 65%ä»¥ä¸Š | 80%ä»¥ä¸Š |

**ç·åˆåŸºæº–**

| æŒ‡æ¨™ | æœ€ä½åŸºæº– | ç›®æ¨™å€¤ | å„ªç§€åŸºæº– |
|------|---------|--------|---------|
| **ç·åˆå¦¥å½“æ€§ã‚¹ã‚³ã‚¢** | 3.5ä»¥ä¸Š | 3.8ä»¥ä¸Š | 4.0ä»¥ä¸Š |
| **ã‚¨ãƒ³ã‚¸ãƒ³é–“å·®** | 0.5ä»¥å†… | 0.3ä»¥å†… | 0.2ä»¥å†… |
| **å…¨ã‚¨ãƒ³ã‚¸ãƒ³åŸºæº–ã‚¯ãƒªã‚¢** | 2/3ã‚¨ãƒ³ã‚¸ãƒ³ | 3/3ã‚¨ãƒ³ã‚¸ãƒ³ | 3/3ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆå„ªç§€åŸºæº–ï¼‰ |

**çµ±è¨ˆçš„åˆ¤å®š**:
- Friedmanæ¤œå®šã§ p â‰¥ 0.05 â†’ **ã‚¨ãƒ³ã‚¸ãƒ³é–“ã«æœ‰æ„å·®ãªã—ï¼ˆãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„ï¼‰**
- å„ã‚¨ãƒ³ã‚¸ãƒ³ãŒåŸºæº–å€¤3.5ã‚ˆã‚Šæœ‰æ„ã«é«˜ã„ï¼ˆp < 0.05ï¼‰â†’ **åŠ¹æœã‚ã‚Š**

---

### Q5: ç·åˆæº€è¶³åº¦

#### ğŸ“‹ ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆæ‰‹æ³•
**CSUQ + ç·åˆæº€è¶³åº¦è©•ä¾¡**

#### ğŸ“ è³ªå•æ–‡
```
Q5. EmoTABIã®ä½“é¨“å…¨ä½“ã«ã¤ã„ã¦ã€ã©ã®ç¨‹åº¦æº€è¶³ã—ã¾ã—ãŸã‹ï¼Ÿ

â–¡ 1 - éå¸¸ã«ä¸æº€
â–¡ 2 - ã‚„ã‚„ä¸æº€
â–¡ 3 - ã©ã¡ã‚‰ã¨ã‚‚ã„ãˆãªã„
â–¡ 4 - ã‚„ã‚„æº€è¶³
â–¡ 5 - éå¸¸ã«æº€è¶³
```

#### ğŸ“Š çµæœåˆ†ææ–¹æ³•

**1. æº€è¶³åº¦æŒ‡æ¨™**
```python
# å¹³å‡æº€è¶³åº¦
mean_satisfaction = df['Q5'].mean()

# æº€è¶³ç‡ï¼ˆã‚¹ã‚³ã‚¢4ä»¥ä¸Šï¼‰
satisfaction_rate = (df['Q5'] >= 4).sum() / len(df) * 100

# NPSé¢¨ã‚¹ã‚³ã‚¢
promoters = (df['Q5'] == 5).sum() / len(df) * 100
detractors = (df['Q5'] <= 2).sum() / len(df) * 100
nps_score = promoters - detractors
```

**2. æº€è¶³åº¦ã«å½±éŸ¿ã™ã‚‹è¦å› åˆ†æï¼ˆé‡å›å¸°åˆ†æï¼‰**
```python
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

# èª¬æ˜å¤‰æ•°ï¼šQ1-Q4
X = df[['Q1', 'Q2', 'Q3', 
        (df['Q4_A'] + df['Q4_B'] + df['Q4_C']) / 3]]
y = df['Q5']

# æ¨™æº–åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# å›å¸°åˆ†æ
model = LinearRegression()
model.fit(X_scaled, y)

# æ¨™æº–åŒ–å›å¸°ä¿‚æ•°ï¼ˆå½±éŸ¿åº¦ï¼‰
coefficients = dict(zip(['æ¨è–¦é©åˆ‡åº¦', 'åˆ°é”åº¦', 'è¡Œå‹•æ„å›³', 'æ„Ÿæƒ…å¦¥å½“æ€§'], 
                        model.coef_))
```

**3. æº€è¶³åº¦ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ**
```python
# æº€è¶³åº¦ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ã®ç‰¹å¾´
high_satisfaction = df[df['Q5'] >= 4]
low_satisfaction = df[df['Q5'] <= 2]

print("é«˜æº€è¶³ç¾¤ã®ç‰¹å¾´:")
print(f"  Q1å¹³å‡: {high_satisfaction['Q1'].mean():.2f}")
print(f"  Q2å¹³å‡: {high_satisfaction['Q2'].mean():.2f}")
print(f"  Q3å¹³å‡: {high_satisfaction['Q3'].mean():.2f}")
```

#### âœ… åŠ¹æœã‚ã‚Šã¨åˆ¤æ–­ã§ãã‚‹åŸºæº–

| æŒ‡æ¨™ | æœ€ä½åŸºæº– | ç›®æ¨™å€¤ | å„ªç§€åŸºæº– |
|------|---------|--------|---------|
| **å¹³å‡æº€è¶³åº¦** | 3.5ä»¥ä¸Š | 3.8ä»¥ä¸Š | 4.0ä»¥ä¸Š |
| **æº€è¶³ç‡** | 60%ä»¥ä¸Š | 70%ä»¥ä¸Š | 80%ä»¥ä¸Š |
| **NPSé¢¨ã‚¹ã‚³ã‚¢** | 20ä»¥ä¸Š | 30ä»¥ä¸Š | 40ä»¥ä¸Š |
| **æ±ºå®šä¿‚æ•° RÂ²** | 0.5ä»¥ä¸Š | 0.6ä»¥ä¸Š | 0.7ä»¥ä¸Š |

**çµ±è¨ˆçš„åˆ¤å®š**:
- å¹³å‡æº€è¶³åº¦ãŒåŸºæº–å€¤3.5ã‚ˆã‚Šæœ‰æ„ã«é«˜ã„ï¼ˆp < 0.05ï¼‰â†’ **åŠ¹æœã‚ã‚Š**
- é‡å›å¸°åˆ†æã®RÂ²ãŒ0.5ä»¥ä¸Š â†’ **æº€è¶³åº¦ã®å¤‰å‹•ã‚’èª¬æ˜ã§ãã¦ã„ã‚‹**

---

## çµ±è¨ˆåˆ†ææ‰‹æ³•

### å¿…é ˆåˆ†æï¼ˆæœ€ä½é™å®Ÿæ–½ã™ã¹ãï¼‰

#### 1. è¨˜è¿°çµ±è¨ˆ
```python
# å„è³ªå•ã®åŸºæœ¬çµ±è¨ˆé‡
for col in ['Q1', 'Q2', 'Q3', 'Q4_A', 'Q4_B', 'Q4_C', 'Q5']:
    print(f"{col}:")
    print(f"  å¹³å‡: {df[col].mean():.2f}")
    print(f"  ä¸­å¤®å€¤: {df[col].median():.1f}")
    print(f"  æ¨™æº–åå·®: {df[col].std():.2f}")
    print(f"  æœ€å°-æœ€å¤§: {df[col].min()}-{df[col].max()}")
```

#### 2. 1ã‚µãƒ³ãƒ—ãƒ«tæ¤œå®šï¼ˆåŸºæº–å€¤ã¨ã®æ¯”è¼ƒï¼‰
```python
from scipy import stats

# å„è³ªå•ãŒåŸºæº–å€¤ã‚’è¶…ãˆã¦ã„ã‚‹ã‹æ¤œè¨¼
baseline = {'Q1': 3.0, 'Q2': 3.0, 'Q3': 3.0, 
            'Q4_A': 3.5, 'Q4_B': 3.5, 'Q4_C': 3.5, 'Q5': 3.5}

for col, base_value in baseline.items():
    t_stat, p_value = stats.ttest_1samp(df[col], base_value)
    print(f"{col}: t={t_stat:.3f}, p={p_value:.4f}")
```

#### 3. ç›¸é–¢åˆ†æ
```python
# Q1ã¨Q3ã®ç›¸é–¢ï¼ˆæ¨è–¦å“è³ªã¨è¡Œå‹•æ„å›³ï¼‰
corr_matrix = df[['Q1', 'Q2', 'Q3', 'Q4_A', 'Q4_B', 'Q4_C', 'Q5']].corr()
print(corr_matrix)
```

#### 4. Friedmanæ¤œå®šï¼ˆæ„Ÿæƒ…ã‚¨ãƒ³ã‚¸ãƒ³é–“æ¯”è¼ƒï¼‰
```python
from scipy.stats import friedmanchisquare

stat, p_value = friedmanchisquare(df['Q4_A'], df['Q4_B'], df['Q4_C'])
print(f"Friedmanæ¤œå®š: Ï‡Â²={stat:.3f}, p={p_value:.4f}")
```

### æ¨å¥¨åˆ†æï¼ˆã‚ˆã‚Šè©³ç´°ãªåˆ†æï¼‰

#### 5. 1è¦å› åˆ†æ•£åˆ†æï¼ˆç”»åƒã‚°ãƒ«ãƒ¼ãƒ—é–“æ¯”è¼ƒï¼‰
```python
# ç”»åƒ1, 2, 3ã§åŠ¹æœã«å·®ãŒã‚ã‚‹ã‹
groups = [df[df['ä½¿ç”¨ç”»åƒ'] == i]['Q1'] for i in [1, 2, 3]]
f_stat, p_value = stats.f_oneway(*groups)

if p_value < 0.05:
    # äº‹å¾Œæ¤œå®šï¼ˆTukey HSDï¼‰
    from statsmodels.stats.multicomp import pairwise_tukeyhsd
    tukey = pairwise_tukeyhsd(df['Q1'], df['ä½¿ç”¨ç”»åƒ'])
    print(tukey)
```

#### 6. åŠ¹æœé‡ã®ç®—å‡º
```python
# Cohen's dï¼ˆå®Ÿè³ªçš„ãªåŠ¹æœã®å¤§ãã•ï¼‰
def cohens_d(data, baseline):
    return (data.mean() - baseline) / data.std()

d_q1 = cohens_d(df['Q1'], 3.0)
print(f"Q1ã®Cohen's d: {d_q1:.3f}")

# è§£é‡ˆ
if abs(d_q1) >= 0.8:
    print("  â†’ å¤§ãã„åŠ¹æœ")
elif abs(d_q1) >= 0.5:
    print("  â†’ ä¸­ç¨‹åº¦ã®åŠ¹æœ")
elif abs(d_q1) >= 0.2:
    print("  â†’ å°ã•ã„åŠ¹æœ")
```

#### 7. é‡å›å¸°åˆ†æï¼ˆæº€è¶³åº¦ã®äºˆæ¸¬ï¼‰
```python
from sklearn.linear_model import LinearRegression

X = df[['Q1', 'Q2', 'Q3', 'Q4_total']]
y = df['Q5']

model = LinearRegression()
model.fit(X, y)

print(f"æ±ºå®šä¿‚æ•° RÂ²: {model.score(X, y):.3f}")
print("å›å¸°ä¿‚æ•°:")
for name, coef in zip(X.columns, model.coef_):
    print(f"  {name}: {coef:.3f}")
```

### ã‚ªãƒ—ã‚·ãƒ§ãƒ³åˆ†æï¼ˆå­¦è¡“è«–æ–‡ãƒ¬ãƒ™ãƒ«ï¼‰

#### 8. ä¿¡é ¼æ€§åˆ†æï¼ˆCronbach's Î±ï¼‰
```python
import pingouin as pg

# Q4ã®3é …ç›®ã®å†…çš„ä¸€è²«æ€§
alpha = pg.cronbach_alpha(df[['Q4_A', 'Q4_B', 'Q4_C']])
print(f"Cronbach's Î±: {alpha[0]:.3f}")

# è§£é‡ˆï¼šÎ± > 0.7ã§ååˆ†ãªä¿¡é ¼æ€§
```

#### 9. å¤šé‡æ¯”è¼ƒã®è£œæ­£
```python
from statsmodels.stats.multitest import multipletests

# è¤‡æ•°ã®tæ¤œå®šã‚’å®Ÿæ–½ã™ã‚‹å ´åˆ
p_values = [0.01, 0.03, 0.04, 0.15]  # ä¾‹
reject, p_corrected, _, _ = multipletests(p_values, method='bonferroni')

print("è£œæ­£å¾Œpå€¤:", p_corrected)
```

---

## åŠ¹æœåˆ¤å®šåŸºæº–

### å€‹åˆ¥è³ªå•ã®åŠ¹æœåˆ¤å®š

| è³ªå• | åŠ¹æœã‚ã‚ŠåŸºæº– |
|------|-------------|
| **Q1** | å¹³å‡3.5ä»¥ä¸Š AND è‚¯å®šç‡60%ä»¥ä¸Š AND p < 0.05 |
| **Q2** | æˆåŠŸç‡70%ä»¥ä¸Š AND p < 0.05ï¼ˆvs 50%ï¼‰ |
| **Q3** | æ„å›³ç‡50%ä»¥ä¸Š AND Q1ã¨ã®ç›¸é–¢r > 0.5, p < 0.05 |
| **Q4** | ç·åˆ3.5ä»¥ä¸Š AND å„ã‚¨ãƒ³ã‚¸ãƒ³åŸºæº–ã‚¯ãƒªã‚¢2/3ä»¥ä¸Š |
| **Q5** | å¹³å‡3.5ä»¥ä¸Š AND æº€è¶³ç‡60%ä»¥ä¸Š AND p < 0.05 |

### ã‚·ã‚¹ãƒ†ãƒ ç·åˆè©•ä¾¡

#### ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—å¼
```python
ç·åˆã‚¹ã‚³ã‚¢ = (
    Q1å¹³å‡ Ã— 0.25 +  # æ¨è–¦é©åˆ‡åº¦: 25%
    Q2å¹³å‡ Ã— 0.20 +  # åˆ°é”åº¦: 20%
    Q3å¹³å‡ Ã— 0.25 +  # è¡Œå‹•æ„å›³: 25%
    Q4ç·åˆ Ã— 0.20 +  # æ„Ÿæƒ…å¦¥å½“æ€§: 20%
    Q5å¹³å‡ Ã— 0.10    # ç·åˆæº€è¶³åº¦: 10%
)
```

#### ã‚·ã‚¹ãƒ†ãƒ æˆç†Ÿåº¦ãƒ¬ãƒ™ãƒ«

| ãƒ¬ãƒ™ãƒ« | ç·åˆã‚¹ã‚³ã‚¢ | åˆ¤å®š | ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ |
|-------|-----------|------|-----------|
| **Lv.5** | 4.0ä»¥ä¸Š | å„ªç§€ â­â­â­â­â­ | å®Ÿç”¨åŒ–æ¨é€²ã€å­¦è¡“ç™ºè¡¨ |
| **Lv.4** | 3.5-3.99 | è‰¯å¥½ â­â­â­â­ | å¾®èª¿æ•´ã—ã¦å±•é–‹ |
| **Lv.3** | 3.0-3.49 | åŠç¬¬ç‚¹ â­â­â­ | æ”¹å–„ãŒå¿…è¦ |
| **Lv.2** | 2.5-2.99 | è¦æ”¹å–„ â­â­ | å¤§å¹…ãªè¦‹ç›´ã— |
| **Lv.1** | 2.5æœªæº€ | ä¸åˆæ ¼ â­ | æ ¹æœ¬çš„ãªå†è¨­è¨ˆ |

### ã‚µãƒ³ãƒ—ãƒ«æ•°ã«ã‚ˆã‚‹ä¿¡é ¼æ€§

| ã‚µãƒ³ãƒ—ãƒ«æ•° | ä¿¡é ¼æ€§ãƒ¬ãƒ™ãƒ« | æ¨å¥¨åˆ†ææ‰‹æ³• |
|-----------|-------------|-------------|
| **n < 15** | å‚è€ƒå€¤ | è¨˜è¿°çµ±è¨ˆã®ã¿ |
| **15 â‰¤ n < 30** | ä½ï½ä¸­ | ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®š |
| **30 â‰¤ n < 50** | ä¸­ | ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®š |
| **50 â‰¤ n** | é«˜ | å…¨ã¦ã®çµ±è¨ˆæ‰‹æ³•ãŒé©ç”¨å¯èƒ½ |

---

## å®Ÿè£…ã‚³ãƒ¼ãƒ‰

### å®Œå…¨ç‰ˆçµ±è¨ˆåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```python
"""
EmoTABIå®Ÿè¨¼å®Ÿé¨“ çµ±è¨ˆåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import friedmanchisquare
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆmatplotlibç”¨ï¼‰
plt.rcParams['font.sans-serif'] = ['Yu Gothic', 'MS Gothic']
plt.rcParams['font.family'] = 'sans-serif'

# ==========================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ==========================================
df = pd.read_csv('emotabi_survey_results.csv')

print("="*60)
print("EmoTABIå®Ÿè¨¼å®Ÿé¨“ çµ±è¨ˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
print("="*60)
print(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(df)}å")
print("="*60)

# ==========================================
# Q1: æ¨è–¦ã®é©åˆ‡åº¦
# ==========================================
print("\nã€Q1: æ¨è–¦ã®é©åˆ‡åº¦ã€‘")
q1_mean = df['Q1'].mean()
q1_std = df['Q1'].std()
q1_median = df['Q1'].median()
q1_positive_rate = (df['Q1'] >= 4).sum() / len(df) * 100

print(f"å¹³å‡å€¤: {q1_mean:.2f} (SD={q1_std:.2f})")
print(f"ä¸­å¤®å€¤: {q1_median:.1f}")
print(f"è‚¯å®šç‡ï¼ˆ4ä»¥ä¸Šï¼‰: {q1_positive_rate:.1f}%")

# 1ã‚µãƒ³ãƒ—ãƒ«tæ¤œå®šï¼ˆåŸºæº–å€¤3.0ã¨ã®æ¯”è¼ƒï¼‰
t_stat, p_value = stats.ttest_1samp(df['Q1'], 3.0)
cohens_d = (q1_mean - 3.0) / q1_std

print(f"\n1ã‚µãƒ³ãƒ—ãƒ«tæ¤œå®šï¼ˆvs åŸºæº–å€¤3.0ï¼‰:")
print(f"  t({len(df)-1}) = {t_stat:.3f}, p = {p_value:.4f}")
print(f"  åŠ¹æœé‡ Cohen's d = {cohens_d:.3f}")

if p_value < 0.05 and t_stat > 0:
    print(f"  âœ… çµ±è¨ˆçš„ã«æœ‰æ„ã«é«˜ã„")
else:
    print(f"  âŒ æœ‰æ„å·®ãªã—")

# åŠ¹æœåˆ¤å®š
if q1_mean >= 3.5 and q1_positive_rate >= 60 and p_value < 0.05:
    print(f"\nâœ… Q1åŠ¹æœåˆ¤å®š: åŠ¹æœã‚ã‚Š")
else:
    print(f"\nâŒ Q1åŠ¹æœåˆ¤å®š: è¦æ”¹å–„")

# ç”»åƒã‚°ãƒ«ãƒ¼ãƒ—é–“æ¯”è¼ƒ
if 'ä½¿ç”¨ç”»åƒ' in df.columns:
    print(f"\nç”»åƒã‚°ãƒ«ãƒ¼ãƒ—é–“æ¯”è¼ƒï¼ˆ1è¦å› åˆ†æ•£åˆ†æï¼‰:")
    groups = [df[df['ä½¿ç”¨ç”»åƒ'] == i]['Q1'] for i in [1, 2, 3]]
    f_stat, p_anova = stats.f_oneway(*groups)
    print(f"  F({2}, {len(df)-3}) = {f_stat:.3f}, p = {p_anova:.4f}")
    
    if p_anova < 0.05:
        print(f"  âœ… ç”»åƒé–“ã§æœ‰æ„å·®ã‚ã‚Š")
    else:
        print(f"  âŒ ç”»åƒé–“ã§æœ‰æ„å·®ãªã—")

# ==========================================
# Q2: æº€è¶³å€™è£œã¸ã®åˆ°é”åº¦
# ==========================================
print("\n\nã€Q2: æº€è¶³å€™è£œã¸ã®åˆ°é”åº¦ã€‘")
q2_mean = df['Q2'].mean()
q2_std = df['Q2'].std()
task_success_rate = (df['Q2'] >= 3).sum() / len(df) * 100
high_satisfaction_rate = (df['Q2'] >= 4).sum() / len(df) * 100

print(f"å¹³å‡å€¤: {q2_mean:.2f} (SD={q2_std:.2f})")
print(f"ã‚¿ã‚¹ã‚¯æˆåŠŸç‡ï¼ˆ3ä»¥ä¸Šï¼‰: {task_success_rate:.1f}%")
print(f"é«˜æº€è¶³ç‡ï¼ˆ4ä»¥ä¸Šï¼‰: {high_satisfaction_rate:.1f}%")

# äºŒé …æ¤œå®šï¼ˆæˆåŠŸç‡ãŒ50%ã‚ˆã‚Šé«˜ã„ã‹ï¼‰
from statsmodels.stats.proportion import binom_test
n_success = (df['Q2'] >= 3).sum()
p_binom = binom_test(n_success, len(df), prop=0.5, alternative='larger')

print(f"\näºŒé …æ¤œå®šï¼ˆæˆåŠŸç‡ > 50%ï¼‰:")
print(f"  p = {p_binom:.4f}")

if p_binom < 0.05:
    print(f"  âœ… æˆåŠŸç‡ãŒ50%ã‚ˆã‚Šæœ‰æ„ã«é«˜ã„")
else:
    print(f"  âŒ æœ‰æ„å·®ãªã—")

# åŠ¹æœåˆ¤å®š
if task_success_rate >= 70 and p_binom < 0.05:
    print(f"\nâœ… Q2åŠ¹æœåˆ¤å®š: åŠ¹æœã‚ã‚Š")
else:
    print(f"\nâŒ Q2åŠ¹æœåˆ¤å®š: è¦æ”¹å–„")

# ==========================================
# Q3: è¡Œå‹•æ„å›³
# ==========================================
print("\n\nã€Q3: è¡Œå‹•æ„å›³ã€‘")
q3_mean = df['Q3'].mean()
q3_std = df['Q3'].std()
intention_rate = (df['Q3'] >= 4).sum() / len(df) * 100
strong_intention_rate = (df['Q3'] == 5).sum() / len(df) * 100

print(f"å¹³å‡å€¤: {q3_mean:.2f} (SD={q3_std:.2f})")
print(f"è¡Œå‹•æ„å›³ç‡ï¼ˆ4ä»¥ä¸Šï¼‰: {intention_rate:.1f}%")
print(f"å¼·æ„å›³ç‡ï¼ˆ5ï¼‰: {strong_intention_rate:.1f}%")

# Q1ã¨Q3ã®ç›¸é–¢åˆ†æ
from scipy.stats import pearsonr
corr_q1_q3, p_corr = pearsonr(df['Q1'], df['Q3'])

print(f"\nQ1-Q3ç›¸é–¢åˆ†æ:")
print(f"  r = {corr_q1_q3:.3f}, p = {p_corr:.4f}")

if p_corr < 0.05:
    if abs(corr_q1_q3) >= 0.7:
        strength = "å¼·ã„"
    elif abs(corr_q1_q3) >= 0.5:
        strength = "ä¸­ç¨‹åº¦ã®"
    elif abs(corr_q1_q3) >= 0.3:
        strength = "å¼±ã„"
    else:
        strength = "éå¸¸ã«å¼±ã„"
    print(f"  âœ… {strength}ç›¸é–¢ã‚ã‚Š")
else:
    print(f"  âŒ æœ‰æ„ãªç›¸é–¢ãªã—")

# åŠ¹æœåˆ¤å®š
if intention_rate >= 50 and corr_q1_q3 > 0.5 and p_corr < 0.05:
    print(f"\nâœ… Q3åŠ¹æœåˆ¤å®š: åŠ¹æœã‚ã‚Š")
else:
    print(f"\nâŒ Q3åŠ¹æœåˆ¤å®š: è¦æ”¹å–„")

# ==========================================
# Q4: æ„Ÿæƒ…åˆ†æã®å¦¥å½“æ€§
# ==========================================
print("\n\nã€Q4: æ„Ÿæƒ…åˆ†æã®å¦¥å½“æ€§ã€‘")

q4a_mean = df['Q4_A'].mean()
q4a_std = df['Q4_A'].std()
q4a_accuracy = (df['Q4_A'] >= 4).sum() / len(df) * 100

q4b_mean = df['Q4_B'].mean()
q4b_std = df['Q4_B'].std()
q4b_accuracy = (df['Q4_B'] >= 4).sum() / len(df) * 100

q4c_mean = df['Q4_C'].mean()
q4c_std = df['Q4_C'].std()
q4c_accuracy = (df['Q4_C'] >= 4).sum() / len(df) * 100

q4_total = (q4a_mean + q4b_mean + q4c_mean) / 3

print(f"è‰²å½©æ„Ÿæƒ…ï¼ˆQ4-Aï¼‰: {q4a_mean:.2f} (SD={q4a_std:.2f}), ä¸€è‡´ç‡={q4a_accuracy:.1f}%")
print(f"ç‰©ä½“æ„Ÿæƒ…ï¼ˆQ4-Bï¼‰: {q4b_mean:.2f} (SD={q4b_std:.2f}), ä¸€è‡´ç‡={q4b_accuracy:.1f}%")
print(f"é›°å›²æ°—æ„Ÿæƒ…ï¼ˆQ4-Cï¼‰: {q4c_mean:.2f} (SD={q4c_std:.2f}), ä¸€è‡´ç‡={q4c_accuracy:.1f}%")
print(f"ç·åˆå¦¥å½“æ€§: {q4_total:.2f}")

# Friedmanæ¤œå®šï¼ˆã‚¨ãƒ³ã‚¸ãƒ³é–“æ¯”è¼ƒï¼‰
stat, p_friedman = friedmanchisquare(df['Q4_A'], df['Q4_B'], df['Q4_C'])

print(f"\nFriedmanæ¤œå®šï¼ˆã‚¨ãƒ³ã‚¸ãƒ³é–“æ¯”è¼ƒï¼‰:")
print(f"  Ï‡Â²({2}) = {stat:.3f}, p = {p_friedman:.4f}")

if p_friedman < 0.05:
    print(f"  âœ… ã‚¨ãƒ³ã‚¸ãƒ³é–“ã§æœ‰æ„å·®ã‚ã‚Š")
    
    # äº‹å¾Œæ¤œå®šï¼ˆWilcoxonç¬¦å·ä»˜é †ä½æ¤œå®š + Bonferroniè£œæ­£ï¼‰
    alpha_corrected = 0.05 / 3
    print(f"\n  äº‹å¾Œæ¤œå®šï¼ˆBonferroniè£œæ­£: Î± = {alpha_corrected:.4f}ï¼‰:")
    
    stat_ab, p_ab = stats.wilcoxon(df['Q4_A'], df['Q4_B'])
    print(f"    è‰²å½© vs ç‰©ä½“: p = {p_ab:.4f} {'*' if p_ab < alpha_corrected else 'n.s.'}")
    
    stat_ac, p_ac = stats.wilcoxon(df['Q4_A'], df['Q4_C'])
    print(f"    è‰²å½© vs é›°å›²æ°—: p = {p_ac:.4f} {'*' if p_ac < alpha_corrected else 'n.s.'}")
    
    stat_bc, p_bc = stats.wilcoxon(df['Q4_B'], df['Q4_C'])
    print(f"    ç‰©ä½“ vs é›°å›²æ°—: p = {p_bc:.4f} {'*' if p_bc < alpha_corrected else 'n.s.'}")
else:
    print(f"  âŒ ã‚¨ãƒ³ã‚¸ãƒ³é–“ã§æœ‰æ„å·®ãªã—ï¼ˆãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„ï¼‰")

# å„ã‚¨ãƒ³ã‚¸ãƒ³ãŒåŸºæº–å€¤3.5ä»¥ä¸Šã‹
print(f"\nå„ã‚¨ãƒ³ã‚¸ãƒ³ã®åŠ¹æœåˆ¤å®šï¼ˆåŸºæº–å€¤3.5ã¨ã®æ¯”è¼ƒï¼‰:")
engine_results = []
for label, col, baseline in [('è‰²å½©', 'Q4_A', 3.5), 
                              ('ç‰©ä½“', 'Q4_B', 3.3), 
                              ('é›°å›²æ°—', 'Q4_C', 3.7)]:
    t, p = stats.ttest_1samp(df[col], baseline)
    result = "âœ… åŠ¹æœã‚ã‚Š" if p < 0.05 and t > 0 else "âŒ è¦æ”¹å–„"
    engine_results.append(p < 0.05 and t > 0)
    print(f"  {label}: t={t:.3f}, p={p:.4f} â†’ {result}")

# ç·åˆåŠ¹æœåˆ¤å®š
engines_pass = sum(engine_results)
if q4_total >= 3.5 and engines_pass >= 2:
    print(f"\nâœ… Q4åŠ¹æœåˆ¤å®š: åŠ¹æœã‚ã‚Šï¼ˆ{engines_pass}/3ã‚¨ãƒ³ã‚¸ãƒ³åˆæ ¼ï¼‰")
else:
    print(f"\nâŒ Q4åŠ¹æœåˆ¤å®š: è¦æ”¹å–„ï¼ˆ{engines_pass}/3ã‚¨ãƒ³ã‚¸ãƒ³åˆæ ¼ï¼‰")

# ==========================================
# Q5: ç·åˆæº€è¶³åº¦
# ==========================================
print("\n\nã€Q5: ç·åˆæº€è¶³åº¦ã€‘")
q5_mean = df['Q5'].mean()
q5_std = df['Q5'].std()
satisfaction_rate = (df['Q5'] >= 4).sum() / len(df) * 100

print(f"å¹³å‡å€¤: {q5_mean:.2f} (SD={q5_std:.2f})")
print(f"æº€è¶³ç‡ï¼ˆ4ä»¥ä¸Šï¼‰: {satisfaction_rate:.1f}%")

# NPSé¢¨ã‚¹ã‚³ã‚¢
promoters = (df['Q5'] == 5).sum() / len(df) * 100
detractors = (df['Q5'] <= 2).sum() / len(df) * 100
nps_score = promoters - detractors

print(f"NPSé¢¨ã‚¹ã‚³ã‚¢: {nps_score:.1f} (æ¨å¥¨è€…{promoters:.1f}% - æ‰¹åˆ¤è€…{detractors:.1f}%)")

# åŸºæº–å€¤3.5ã¨ã®æ¯”è¼ƒ
t_stat, p_value = stats.ttest_1samp(df['Q5'], 3.5)

print(f"\n1ã‚µãƒ³ãƒ—ãƒ«tæ¤œå®šï¼ˆvs åŸºæº–å€¤3.5ï¼‰:")
print(f"  t({len(df)-1}) = {t_stat:.3f}, p = {p_value:.4f}")

if p_value < 0.05 and t_stat > 0:
    print(f"  âœ… çµ±è¨ˆçš„ã«æœ‰æ„ã«é«˜ã„")
else:
    print(f"  âŒ æœ‰æ„å·®ãªã—")

# åŠ¹æœåˆ¤å®š
if q5_mean >= 3.5 and satisfaction_rate >= 60 and p_value < 0.05:
    print(f"\nâœ… Q5åŠ¹æœåˆ¤å®š: åŠ¹æœã‚ã‚Š")
else:
    print(f"\nâŒ Q5åŠ¹æœåˆ¤å®š: è¦æ”¹å–„")

# ==========================================
# é‡å›å¸°åˆ†æï¼šæº€è¶³åº¦ã«å½±éŸ¿ã™ã‚‹è¦å› 
# ==========================================
print("\n\nã€é‡å›å¸°åˆ†æï¼šQ5ã«å½±éŸ¿ã™ã‚‹è¦å› ã€‘")

X = df[['Q1', 'Q2', 'Q3']].copy()
X['Q4_total'] = (df['Q4_A'] + df['Q4_B'] + df['Q4_C']) / 3
y = df['Q5']

# æ¨™æº–åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

model = LinearRegression()
model.fit(X_scaled, y)

r_squared = model.score(X_scaled, y)

print(f"æ±ºå®šä¿‚æ•° RÂ² = {r_squared:.3f}")
print(f"\næ¨™æº–åŒ–å›å¸°ä¿‚æ•°ï¼ˆå½±éŸ¿åº¦ã®å¤§ãã•ï¼‰:")

factor_names = ['Q1_æ¨è–¦é©åˆ‡åº¦', 'Q2_åˆ°é”åº¦', 'Q3_è¡Œå‹•æ„å›³', 'Q4_æ„Ÿæƒ…å¦¥å½“æ€§']
for name, coef in zip(factor_names, model.coef_):
    print(f"  {name}: Î² = {coef:+.3f}")

max_idx = np.argmax(np.abs(model.coef_))
print(f"\nâœ… æœ€ã‚‚å½±éŸ¿ãŒå¤§ãã„è¦å› : {factor_names[max_idx]}")

# ==========================================
# ç·åˆè©•ä¾¡
# ==========================================
print("\n\n" + "="*60)
print("ã€ç·åˆè©•ä¾¡ã€‘")
print("="*60)

total_score = (
    q1_mean * 0.25 +
    q2_mean * 0.20 +
    q3_mean * 0.25 +
    q4_total * 0.20 +
    q5_mean * 0.10
)

print(f"\nç·åˆã‚¹ã‚³ã‚¢: {total_score:.2f} / 5.0")
print(f"\nã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡:")

if total_score >= 4.0:
    level = "â­â­â­â­â­ ãƒ¬ãƒ™ãƒ«5: å„ªç§€"
    action = "å®Ÿç”¨åŒ–æ¨é€²ã€å­¦è¡“ç™ºè¡¨ã‚’æ¨å¥¨"
elif total_score >= 3.5:
    level = "â­â­â­â­ ãƒ¬ãƒ™ãƒ«4: è‰¯å¥½"
    action = "å¾®èª¿æ•´ã—ã¦å±•é–‹å¯èƒ½"
elif total_score >= 3.0:
    level = "â­â­â­ ãƒ¬ãƒ™ãƒ«3: åŠç¬¬ç‚¹"
    action = "æ”¹å–„ãŒå¿…è¦"
elif total_score >= 2.5:
    level = "â­â­ ãƒ¬ãƒ™ãƒ«2: è¦æ”¹å–„"
    action = "å¤§å¹…ãªè¦‹ç›´ã—ãŒå¿…è¦"
else:
    level = "â­ ãƒ¬ãƒ™ãƒ«1: ä¸åˆæ ¼"
    action = "æ ¹æœ¬çš„ãªå†è¨­è¨ˆãŒå¿…è¦"

print(f"  {level}")
print(f"  æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {action}")

# å€‹åˆ¥è©•ä¾¡ã‚µãƒãƒªãƒ¼
print(f"\nå€‹åˆ¥è³ªå•ã®åŠ¹æœåˆ¤å®šã‚µãƒãƒªãƒ¼:")
q_results = []
q_results.append(("Q1_æ¨è–¦é©åˆ‡åº¦", q1_mean >= 3.5 and q1_positive_rate >= 60))
q_results.append(("Q2_åˆ°é”åº¦", task_success_rate >= 70))
q_results.append(("Q3_è¡Œå‹•æ„å›³", intention_rate >= 50))
q_results.append(("Q4_æ„Ÿæƒ…å¦¥å½“æ€§", q4_total >= 3.5))
q_results.append(("Q5_ç·åˆæº€è¶³åº¦", q5_mean >= 3.5))

for q_name, passed in q_results:
    status = "âœ… åˆæ ¼" if passed else "âŒ ä¸åˆæ ¼"
    print(f"  {q_name}: {status}")

pass_count = sum([r[1] for r in q_results])
print(f"\nåˆæ ¼é …ç›®: {pass_count}/5")

print("\n" + "="*60)
print("åˆ†æå®Œäº†")
print("="*60)

# ==========================================
# å¯è¦–åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
# ==========================================
def create_visualizations():
    """çµæœã®å¯è¦–åŒ–"""
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    
    # Q1-Q5ã®åˆ†å¸ƒ
    for i, col in enumerate(['Q1', 'Q2', 'Q3', 'Q5']):
        ax = axes[i // 3, i % 3]
        df[col].value_counts().sort_index().plot(kind='bar', ax=ax)
        ax.set_title(f'{col}ã®åˆ†å¸ƒ')
        ax.set_xlabel('ã‚¹ã‚³ã‚¢')
        ax.set_ylabel('äººæ•°')
    
    # Q4ã®3ã‚¨ãƒ³ã‚¸ãƒ³æ¯”è¼ƒ
    ax = axes[1, 1]
    q4_data = df[['Q4_A', 'Q4_B', 'Q4_C']].mean()
    q4_data.plot(kind='bar', ax=ax)
    ax.set_title('Q4: æ„Ÿæƒ…åˆ†æã‚¨ãƒ³ã‚¸ãƒ³æ¯”è¼ƒ')
    ax.set_xticklabels(['è‰²å½©', 'ç‰©ä½“', 'é›°å›²æ°—'], rotation=0)
    ax.set_ylabel('å¹³å‡ã‚¹ã‚³ã‚¢')
    ax.axhline(y=3.5, color='r', linestyle='--', label='åŸºæº–å€¤')
    ax.legend()
    
    # ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
    ax = axes[1, 2]
    corr_matrix = df[['Q1', 'Q2', 'Q3', 'Q4_A', 'Q4_B', 'Q4_C', 'Q5']].corr()
    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', 
                center=0, ax=ax, cbar_kws={'label': 'ç›¸é–¢ä¿‚æ•°'})
    ax.set_title('è³ªå•é–“ã®ç›¸é–¢')
    
    plt.tight_layout()
    plt.savefig('emotabi_analysis_results.png', dpi=300, bbox_inches='tight')
    print("\nå¯è¦–åŒ–çµæœã‚’ 'emotabi_analysis_results.png' ã«ä¿å­˜ã—ã¾ã—ãŸ")

# å¯è¦–åŒ–å®Ÿè¡Œï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’å¤–ã™ã¨å®Ÿè¡Œï¼‰
# create_visualizations()
```

### CSVãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¾‹

```csv
å›ç­”è€…ID,ä½¿ç”¨ç”»åƒ,Q1,Q2,Q3,Q4_A,Q4_B,Q4_C,Q5
1,1,4,3,4,4,3,5,4
2,2,5,4,5,5,4,4,5
3,1,3,3,3,3,3,4,3
4,3,4,4,4,4,4,5,4
5,2,5,5,5,5,5,5,5
...
```

---

## å®Ÿæ–½æ‰‹é †

### 1. äº‹å‰æº–å‚™
- [ ] ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãƒ•ã‚©ãƒ¼ãƒ ã®ä½œæˆï¼ˆGoogle Forms / Microsoft Formsç­‰ï¼‰
- [ ] ã‚µãƒ³ãƒ—ãƒ«ç”»åƒ1, 2, 3ã®é¸å®š
- [ ] è¢«é¨“è€…ã®ãƒªã‚¯ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼ˆç›®æ¨™: n â‰¥ 30ï¼‰
- [ ] å€«ç†å¯©æŸ»ã®ç¢ºèªï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰

### 2. å®Ÿé¨“å®Ÿæ–½
- [ ] è¢«é¨“è€…ã«EmoTABIã‚’ä½¿ç”¨ã—ã¦ã‚‚ã‚‰ã†
- [ ] ä½¿ç”¨ç›´å¾Œã«ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”
- [ ] ãƒ‡ãƒ¼ã‚¿ã®è¨˜éŒ²ãƒ»ä¿å­˜

### 3. ãƒ‡ãƒ¼ã‚¿åˆ†æ
- [ ] CSVãƒ‡ãƒ¼ã‚¿ã®æ•´å½¢
- [ ] çµ±è¨ˆåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œ
- [ ] çµæœã®å¯è¦–åŒ–

### 4. çµæœå ±å‘Š
- [ ] åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ
- [ ] åŠ¹æœåˆ¤å®šã®å®Ÿæ–½
- [ ] æ”¹å–„ææ¡ˆã®ç­–å®š

---

## å‚è€ƒæ–‡çŒ®

1. **ResQue**: Pu, P., Chen, L., & Hu, R. (2011). "A user-centric evaluation framework for recommender systems." ACM RecSys.

2. **SUS**: Brooke, J. (1996). "SUS: A quick and dirty usability scale." Usability Evaluation in Industry.

3. **TAM**: Davis, F. D. (1989). "Perceived usefulness, perceived ease of use, and user acceptance of information technology." MIS Quarterly.

4. **SDæ³•**: Osgood, C. E., Suci, G., & Tannenbaum, P. (1957). "The measurement of meaning." University of Illinois Press.

5. **CSUQ**: Lewis, J. R. (1995). "IBM Computer Usability Satisfaction Questionnaires: Psychometric Evaluation and Instructions for Use." International Journal of Human-Computer Interaction.

---

## ä»˜éŒ²

### A. è³ªå•ç¥¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆå°åˆ·ç”¨ï¼‰

[åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«: questionnaire_template.pdf ã¨ã—ã¦ä½œæˆæ¨å¥¨]

### B. ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ç”¨Excelãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

[åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«: data_entry_template.xlsx ã¨ã—ã¦ä½œæˆæ¨å¥¨]

### C. åˆ†æçµæœå ±å‘Šæ›¸ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

[åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«: analysis_report_template.docx ã¨ã—ã¦ä½œæˆæ¨å¥¨]

---

**ä½œæˆæ—¥**: 2025å¹´10æœˆ20æ—¥  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0  
**ä½œæˆè€…**: EmoTABIé–‹ç™ºãƒãƒ¼ãƒ 

